---
title: "p8105_hw3_ly2565"
author: "Lin Yang"
date: "10/17/2021"
output: github_document
---

```{r, setup, include = FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## Load the instacart data.

```{r}
data("instacart")
instacart
```

* The data set has `r nrow(instacart)` rows and `r ncol(instacart)` columns with variables: `r names(instacart)`. 

## Some questions related to this data set.
How many aisles are there, and which aisles are the most items ordered from:
```{r}
aisle_count = 
  instacart %>% 
  select(aisle) %>% 
  n_distinct()
aisle_count

aisle_most = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  mutate(aisle_rank = min_rank(desc(n_obs))) %>% 
  filter(aisle_rank < 3)
aisle_most
```

* There are `r aisle_count` aisles in total, and the aisles that the most items are ordered from are `r pull(aisle_most, aisle) `.

Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered:
```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
  mutate(aisle = fct_reorder(factor(aisle), n_obs)) %>% 
  ggplot(aes(x = n_obs, y = aisle)) +
  geom_point() +
  labs(
    title = "Number of Items Ordered from Aisles with over 10000 Items",
    x = "Number of Items",
    y = "Aisles",
    caption = "Data from the Instacart Online Grocery Shopping Dataset 2017")
```

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”:
```{r}
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_items = n()) %>% 
  mutate(product_rank = min_rank(desc(n_items))) %>% 
  filter(product_rank < 4) %>% 
  group_by(aisle) %>% 
  arrange(-n_items) %>% 
  select(-product_rank) %>% 
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:
```{r}
mean_hour_dow =
  instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )  
  
colnames(mean_hour_dow) = c("", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")
mean_hour_dow %>% 
  knitr::kable()
```


# Problem 2

## Load the BRFSS data set.
```{r}
data("brfss_smart2010")
brfss_df = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  separate(locationdesc, into = c('state', 'location'), sep = "-") %>%
  select(-state) %>%
  rename(state = locationabbr) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(response)
brfss_df
```

## Some questions related to the data set.

In 2002, which states were observed at 7 or more locations? What about in 2010:
```{r}
brfss_2002 = 
  brfss_df %>% 
  filter(year == 2002) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)

brfss_2010 = 
  brfss_df %>% 
  filter(year == 2010) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)
```
In 2002, the states observed at 7 or more locations were `r pull(brfss_2002, state)`. In 2010, they were `r pull(brfss_2010, state)`.

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state:
```{r}
excellent_df = 
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(data_value_mean = mean(data_value, na.rm = TRUE))
excellent_df
```
* The excellent response data set contains average data values of `r n_distinct(pull(excellent_df, state))` states (including DC) from `r min(pull(excellent_df, year))` to `r max(pull(excellent_df, year))`. 

Make a “spaghetti” plot of this average value over time within a state:
```{r}
## need more aes and comments
excellent_df %>% 
  ggplot(aes(x = year, y = data_value_mean, group = state, color = state)) +
  geom_line() +
  labs(
    title = "Average Data Value Over Time Within A State",
    x = "Year",
    y = "Average Data Value"
  )
```

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State:
```{r}
ny_2006 = 
  brfss_df %>% 
  filter(year == 2006, state == "NY") %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Data Value in 2006",
    x = "Response",
    y = "Data Value"
  )

ny_2010 = 
  brfss_df %>% 
  filter(year == 2010, state == "NY") %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Data Value in 2010",
    x = "Response",
    y = "Data Value"
  )

ny_2006 + ny_2010

ny_2006 + ny_2010
```
* From the plot, we can see that data values for positive responses(excellent, very good, good) were greater than those for fair and poor responses in both 2006 and 2010.

# Problem 3

## Load and tidy the accelerometer data set.

```{r}
accelerometer_df = 
  read_csv(file = "data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekday_or_end = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday")) %>% 
    select(day_id, week, day, weekday_or_end, everything()) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(minute = as.numeric(minute))

accelerometer_df
```
* The resulting data set has `r nrow(accelerometer_df)` observations of `r ncol(accelerometer_df)` variables: `r names(accelerometer_df)`. In the `minute` column, `1` means the first minute starting at midnight, `2` means the second minute and so on. The values in the `activity_count` column are the activity counts for each minute. 

## Aggregate activity counts for each day.

```{r}
accelerometer_df %>% 
  group_by(week, day) %>% 
  summarize(activity_total = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = activity_total
  ) %>% 
  select(week, Monday, Tuesday, Wednesday, Thursday, Friday, everything()) %>% 
  knitr::kable()
```
* It seems total activities increased during weekdays, but the trend is not strictly increasing. In addition, the total activities on Saturday in week 4 and 5 dropped significantly, which may need us to pay attention to. 

## Make a plot showing the 24-hour activity time courses for each day.

```{r}
accelerometer_df %>% 
  ggplot(aes(x = minute, y = activity_count, color = day, group = day_id)) +
  geom_line(alpha = 0.6) + 
  labs(
    title = "24-hour Activity Counts",
    x = "Time",
    y = "Activity Counts"
  ) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440), 
    labels = c("12am.", "4am", "8am", "12pm", "4pm", "8pm", "11:59pm"),
    limits = c(0, 1440)
  )
```
* From the plot, we can see that activity counts tend to be higher in 3 periods of time on each day: 6am-12pm, 4pm-6pm, and 8pm-10pm(especially on Wed and Fri); activity counts tend to be lower from 10pm-6pm each day which may be the sleeping period. 







