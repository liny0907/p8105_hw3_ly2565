---
title: "p8105_hw3_ly2565"
author: "Lin Yang"
date: "10/17/2021"
output: github_document
---

```{r, setup}
library(tidyverse)
library(ggridges)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## Load the instacart data.

```{r}
data("instacart")
instacart
```

* The data set has `r nrow(instacart)` rows and `r ncol(instacart)` columns with variables: `r names(instacart)`. 

## Some questions related to this data set.
How many aisles are there, and which aisles are the most items ordered from:
```{r}
aisle_count = 
  instacart %>% 
  select(aisle) %>% 
  n_distinct()
aisle_count

aisle_most = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  mutate(aisle_rank = min_rank(desc(n_obs))) %>% 
  filter(aisle_rank < 3)
aisle_most
```

* There are `r aisle_count` aisles in total, and the aisles that the most items are ordered from are `r pull(aisle_most, aisle) `.

Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered:
```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
  mutate(aisle = fct_reorder(factor(aisle), n_obs)) %>% 
  ggplot(aes(x = n_obs, y = aisle)) +
  geom_point() +
  labs(
    title = "Number of Items Ordered from Aisles with over 10000 Items",
    x = "Number of Items",
    y = "Aisles",
    caption = "Data from the Instacart Online Grocery Shopping Dataset 2017")
```

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”:
```{r}
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_items = n()) %>% 
  mutate(product_rank = min_rank(desc(n_items))) %>% 
  filter(product_rank < 4) %>% 
  group_by(aisle) %>% 
  arrange(-n_items) %>% 
  select(-product_rank) %>% 
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:
```{r}
mean_hour_dow =
  instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )  
  
colnames(mean_hour_dow) = c("", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")
mean_hour_dow %>% 
  knitr::kable()
```


# Problem 2

## Load the BRFSS data set.
```{r}
data("brfss_smart2010")
brfss_df = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  separate(locationdesc, into = c('state', 'location'), sep = "-") %>%
  select(-state) %>%
  rename(state = locationabbr) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(response)
brfss_df
```

## Some questions related to the data set.

In 2002, which states were observed at 7 or more locations? What about in 2010:
```{r}
brfss_2002 = 
  brfss_df %>% 
  filter(year == 2002) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)

brfss_2010 = 
  brfss_df %>% 
  filter(year == 2010) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)
```
In 2002, the states observed at 7 or more locations were `r pull(brfss_2002, state)`. In 2010, they were `r pull(brfss_2010, state)`.

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state:
```{r}
excellent_df = 
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(data_value_mean = mean(data_value, na.rm = TRUE))
excellent_df
```
* The excellent response data set contains average data values of `r n_distinct(pull(excellent_df, state))` states (including DC) from `r min(pull(excellent_df, year))` to `r max(pull(excellent_df, year))`. 

Make a “spaghetti” plot of this average value over time within a state:
```{r}
## need more aes and comments
excellent_df %>% 
  ggplot(aes(x = year, y = data_value_mean, group = state, color = state)) +
  geom_line() +
  labs(
    title = "Average Data Value Over Time Within A State",
    x = "Year",
    y = "Average Data Value"
  )
```

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State:
```{r}
ny_2006 = 
  brfss_df %>% 
  filter(year == 2006, state == "NY") %>% 
  ggplot(aes(x = data_value, y = response)) +
  geom_density_ridges(scale = .85) +
  labs(
    title = "Distribution of Data Value in 2006",
    x = "Data Value",
    y = "Response"
  )

ny_2010 = 
  brfss_df %>% 
  filter(year == 2010, state == "NY") %>% 
  ggplot(aes(x = data_value, y = response)) +
  geom_density_ridges(scale = .85) +
  labs(
    title = "Distribution of Data Value in 2010",
    x = "Data Value",
    y = "Response"
  )

ny_2006 + ny_2010
```







