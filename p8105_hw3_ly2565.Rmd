---
title: "p8105_hw3_ly2565"
author: "Lin Yang"
date: "10/17/2021"
output: github_document
---

```{r, setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## Load the instacart data.

```{r}
data("instacart")
instacart
```
* The data set contains `r nrow(instacart)` observations of `r ncol(instacart)` variables: `r names(instacart)`. In total, there are `r n_distinct(pull(instacart, department))` departments and `r n_distinct(pull(instacart, user_id))` unique users. In the `reordered` column, `1` means this product was ordered by this user in the past, `0` means this product was not. 

## Some questions related to this data set.

#### How many aisles are there, and which aisles are the most items ordered from:
```{r}
aisle_count = 
  instacart %>% 
  select(aisle) %>% 
  n_distinct()
aisle_count

aisle_most = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  mutate(aisle_rank = min_rank(desc(n_obs))) %>% 
  filter(aisle_rank < 3)
aisle_most
```

* There are `r aisle_count` aisles in total, and the aisles that the most items are ordered from are `r pull(aisle_most, aisle) `.

#### Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered:
```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
  mutate(aisle = fct_reorder(factor(aisle), n_obs)) %>% 
  ggplot(aes(x = n_obs, y = aisle)) +
  geom_point() +
  labs(
    title = "Aisle Sale Plot",
    x = "Number of Items Ordered",
    y = "Aisles",
    caption = "Data from the Instacart Online Grocery Shopping Dataset 2017") +
  scale_x_continuous(
    breaks = c(20000, 40000, 60000, 80000, 100000, 120000, 140000, 160000), 
    limits = c(0, 160000)
  )
```

* From this plot, we can clearly see that fresh vegetables and fresh fruits are the two most popular aisles. And they are the only two aisles with over 100000 items ordered. 

#### Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”:
```{r, message = FALSE, warning = FALSE}
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_items = n()) %>% 
  mutate(product_rank = min_rank(desc(n_items))) %>% 
  filter(product_rank < 4) %>% 
  arrange(aisle, product_rank) %>% 
  select(-product_rank) %>% 
  knitr::kable()
```
* In the `baking ingredients` aisle, the three most popular items are light brown sugar, pure baking soda, and organic vanilla extract; for `dog food care` aisle, they are organic grain free chicken & vegetable dog food, organic chicken & brown rice recipe, and original dry dog; for `packaged vegetables fruits` aisle, they are organic baby spinach, organic raspberries, and organic blueberries. 


#### Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:
```{r, message = FALSE, warning = FALSE}
mean_hour_dow =
  instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) 
  
colnames(mean_hour_dow) = c("", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")
mean_hour_dow %>% 
  knitr::kable()
```
* According to this table, the mean hour of the day at which coffee ice cream are ordered is higher than that at which pink lady apples are order on all days of the week, except Friday.

# Problem 2

## Load and tidy the BRFSS data set.
```{r, warning = FALSE}
data("brfss_smart2010")
brfss_df = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  separate(locationdesc, into = c('state', 'location'), sep = "-") %>%
  select(-state) %>%
  rename(state = locationabbr) %>% 
  filter(
    topic == "Overall Health",
    response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(response)
brfss_df
```
* This data set has `r nrow(brfss_df)` rows and `r ncol(brfss_df)` columns, focusing on the overall health topic from `r min(pull(brfss_df, year))` to `r max(pull(brfss_df, year))`.

## Some questions related to the data set.

#### In 2002, which states were observed at 7 or more locations? What about in 2010:
```{r}
brfss_2002 = 
  brfss_df %>% 
  filter(year == 2002) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)

brfss_2010 = 
  brfss_df %>% 
  filter(year == 2010) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)
```
* In 2002, there were `r length(pull(brfss_2002, state))` states observed at 7 or more locations, they were `r pull(brfss_2002, state)`. In 2010, there were `r length(pull(brfss_2010, state))` states observed at 7 or more locations: `r pull(brfss_2010, state)`.

#### Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state:
```{r, message = FALSE}
excellent_df = 
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(data_value_mean = mean(data_value, na.rm = TRUE))
excellent_df
```
* The excellent response data set contains average data values of `r n_distinct(pull(excellent_df, state))` states (including DC) from `r min(pull(excellent_df, year))` to `r max(pull(excellent_df, year))`. 

#### Make a “spaghetti” plot of this average value over time within a state:
```{r}
excellent_df %>% 
  ggplot(aes(x = year, y = data_value_mean, group = state, color = state)) +
  geom_line(alpha = 0.8) +
  labs(
    title = "Average Data Values Over Time",
    x = "Year",
    y = "Average Data Values",
    color = "State"
  ) +
  theme(legend.position = "right")
```

* The plot shows that average data values of all states fluctuate from 2002 to 2010. Overall, there is a slight decreasing trend.  

#### Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State:
```{r}
brfss_df %>% 
  filter(year %in% c(2006, 2010), state == "NY") %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Data Values for Responses in NY State",
    x = "Response",
    y = "Data Values"
  ) +
  facet_grid(. ~ year)
```

* From the plot, we can see that data values for positive responses(excellent, very good, good) were greater than those for fair and poor responses in both 2006 and 2010.

# Problem 3

## Load and tidy the accelerometer data set.

```{r}
accelerometer_df = 
  read_csv(file = "data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekday_or_end = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday")) %>% 
    select(day_id, week, day, weekday_or_end, everything()) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(minute = as.numeric(minute))

accelerometer_df
```
* The resulting data set has `r nrow(accelerometer_df)` observations of `r ncol(accelerometer_df)` variables: `r names(accelerometer_df)`. In the `minute` column, `1` means the first minute starting at midnight, `2` means the second minute and so on. The values in the `activity_count` column are the activity counts for each minute. 

## Aggregate activity counts for each day.

```{r, message = FALSE}
accelerometer_df %>% 
  group_by(week, day) %>% 
  summarize(activity_total = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = activity_total
  ) %>% 
  select(week, Monday, Tuesday, Wednesday, Thursday, Friday, everything()) %>% 
  knitr::kable()
```
* It seems total activities increased during weekdays, but the trend is not strictly increasing. In addition, the total activities on Saturday in week 4 and 5 dropped significantly, which may need us to pay attention to. 

## Make a plot showing the 24-hour activity time courses for each day.

```{r, message = FALSE}
accelerometer_df %>% 
  group_by(day, minute) %>% 
  summarize(avg_activity = mean(activity_count)) %>% 
  ggplot(aes(x = minute, y = avg_activity, color = day)) +
  geom_line(alpha = 0.6) + 
  labs(
    title = "Average 24-hour Activity Time Courses",
    x = "Time",
    y = "Average Activity",
    color = "Day"
  ) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440), 
    labels = c("12am", "4am", "8am", "12pm", "4pm", "8pm", "11:59pm"),
    limits = c(0, 1440)
  )
```

* From the plot, we can see that of the 5 weeks, the average activity counts of each day tend to be lower from 10pm to 6am which may be the sleeping period; the average activity tend to be higher in 3 periods of time on each day: 6am-12pm, 4pm-6pm, and 8pm-10pm. Especially on Fridays, this man was much more active from 8pm to 10pm, and on Sundays, he was much more active from 10am-12pm.







