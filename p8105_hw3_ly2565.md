p8105\_hw3\_ly2565
================
Lin Yang
10/17/2021

# Problem 1

## Load the instacart data.

``` r
data("instacart")
instacart
```

    ## # A tibble: 1,384,617 × 15
    ##    order_id product_id add_to_cart_order reordered user_id eval_set order_number
    ##       <int>      <int>             <int>     <int>   <int> <chr>           <int>
    ##  1        1      49302                 1         1  112108 train               4
    ##  2        1      11109                 2         1  112108 train               4
    ##  3        1      10246                 3         0  112108 train               4
    ##  4        1      49683                 4         0  112108 train               4
    ##  5        1      43633                 5         1  112108 train               4
    ##  6        1      13176                 6         0  112108 train               4
    ##  7        1      47209                 7         0  112108 train               4
    ##  8        1      22035                 8         1  112108 train               4
    ##  9       36      39612                 1         0   79431 train              23
    ## 10       36      19660                 2         1   79431 train              23
    ## # … with 1,384,607 more rows, and 8 more variables: order_dow <int>,
    ## #   order_hour_of_day <int>, days_since_prior_order <int>, product_name <chr>,
    ## #   aisle_id <int>, department_id <int>, aisle <chr>, department <chr>

-   The data set contains 1384617 observations of 15 variables:
    order\_id, product\_id, add\_to\_cart\_order, reordered, user\_id,
    eval\_set, order\_number, order\_dow, order\_hour\_of\_day,
    days\_since\_prior\_order, product\_name, aisle\_id, department\_id,
    aisle, department. In total, there are 21 departments and 131209
    unique users. In the `reordered` column, `1` means this product was
    ordered by this user in the past, `0` means this product was not.

## Some questions related to this data set.

#### How many aisles are there, and which aisles are the most items ordered from:

``` r
aisle_count = 
  instacart %>% 
  select(aisle) %>% 
  n_distinct()
aisle_count
```

    ## [1] 134

``` r
aisle_most = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  mutate(aisle_rank = min_rank(desc(n_obs))) %>% 
  filter(aisle_rank < 3)
aisle_most
```

    ## # A tibble: 2 × 3
    ##   aisle             n_obs aisle_rank
    ##   <chr>             <int>      <int>
    ## 1 fresh fruits     150473          2
    ## 2 fresh vegetables 150609          1

-   There are 134 aisles in total, and the aisles that the most items
    are ordered from are fresh fruits, fresh vegetables.

#### Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered:

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
  mutate(aisle = fct_reorder(factor(aisle), n_obs)) %>% 
  ggplot(aes(x = n_obs, y = aisle)) +
  geom_point() +
  labs(
    title = "Aisle Sale Plot",
    x = "Number of Items Ordered",
    y = "Aisles",
    caption = "Data from the Instacart Online Grocery Shopping Dataset 2017") +
  scale_x_continuous(
    breaks = c(20000, 40000, 60000, 80000, 100000, 120000, 140000, 160000), 
    limits = c(0, 160000)
  )
```

<img src="p8105_hw3_ly2565_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

-   From this plot, we can clearly see that fresh vegetables and fresh
    fruits are the two most popular aisles. And they are the only two
    aisles with over 100000 items ordered.

#### Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”:

``` r
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_items = n()) %>% 
  mutate(product_rank = min_rank(desc(n_items))) %>% 
  filter(product_rank < 4) %>% 
  arrange(aisle, product_rank) %>% 
  select(-product_rank) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                   | n\_items |
|:---------------------------|:------------------------------------------------|---------:|
| baking ingredients         | Light Brown Sugar                               |      157 |
| baking ingredients         | Pure Baking Soda                                |      140 |
| baking ingredients         | Organic Vanilla Extract                         |      122 |
| dog food care              | Organix Grain Free Chicken & Vegetable Dog Food |       14 |
| dog food care              | Organix Chicken & Brown Rice Recipe             |       13 |
| dog food care              | Original Dry Dog                                |        9 |
| packaged vegetables fruits | Organic Baby Spinach                            |     3324 |
| packaged vegetables fruits | Organic Raspberries                             |     1920 |
| packaged vegetables fruits | Organic Blueberries                             |     1692 |

-   In the `baking ingredients` aisle, the three most popular items are
    light brown sugar, pure baking soda, and organic vanilla extract;
    for `dog food care` aisle, they are organic grain free chicken &
    vegetable dog food, organic chicken & brown rice recipe, and
    original dry dog; for `packaged vegetables fruits` aisle, they are
    organic baby spinach, organic raspberries, and organic blueberries.

#### Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:

``` r
mean_hour_dow =
  instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) 
  
colnames(mean_hour_dow) = c("", "Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")
mean_hour_dow %>% 
  knitr::kable()
```

|                  |      Sun |      Mon |      Tue |     Wed |      Thu |      Fri |      Sat |
|:-----------------|---------:|---------:|---------:|--------:|---------:|---------:|---------:|
| Coffee Ice Cream | 13.22222 | 15.00000 | 15.33333 | 15.4000 | 15.16667 | 10.33333 | 12.35294 |
| Pink Lady Apples | 12.25000 | 11.67857 | 12.00000 | 13.9375 | 11.90909 | 13.86957 | 11.55556 |

-   According to this table, the mean hour of the day at which coffee
    ice cream are ordered is higher than that at which pink lady apples
    are order on all days of the week, except Friday.

# Problem 2

## Load and tidy the BRFSS data set.

``` r
data("brfss_smart2010")
brfss_df = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  separate(locationdesc, into = c('state', 'location'), sep = "-") %>%
  select(-state) %>%
  rename(state = locationabbr) %>% 
  filter(
    topic == "Overall Health",
    response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(response)
brfss_df
```

    ## # A tibble: 10,625 × 23
    ##     year state location  class  topic  question  response sample_size data_value
    ##    <int> <chr> <chr>     <chr>  <chr>  <chr>     <fct>          <int>      <dbl>
    ##  1  2010 AL    " Jeffer… Healt… Overa… How is y… Poor              45        5.5
    ##  2  2010 AL    " Mobile… Healt… Overa… How is y… Poor              66        6.4
    ##  3  2010 AL    " Tuscal… Healt… Overa… How is y… Poor              35        4.2
    ##  4  2010 AZ    " Marico… Healt… Overa… How is y… Poor              62        3.5
    ##  5  2010 AZ    " Pima C… Healt… Overa… How is y… Poor              49        5.7
    ##  6  2010 AZ    " Pinal … Healt… Overa… How is y… Poor              30        4  
    ##  7  2010 AR    " Benton… Healt… Overa… How is y… Poor              21        3  
    ##  8  2010 AR    " Pulask… Healt… Overa… How is y… Poor              36        3.8
    ##  9  2010 AR    " Washin… Healt… Overa… How is y… Poor              16        2.4
    ## 10  2010 CA    " Alamed… Healt… Overa… How is y… Poor              23        2.4
    ## # … with 10,615 more rows, and 14 more variables: confidence_limit_low <dbl>,
    ## #   confidence_limit_high <dbl>, display_order <int>, data_value_unit <chr>,
    ## #   data_value_type <chr>, data_value_footnote_symbol <chr>,
    ## #   data_value_footnote <chr>, data_source <chr>, class_id <chr>,
    ## #   topic_id <chr>, location_id <chr>, question_id <chr>, respid <chr>,
    ## #   geo_location <chr>

-   This data set has 10625 rows and 23 columns, focusing on the overall
    health topic from 2002 to 2010.

## Some questions related to the data set.

#### In 2002, which states were observed at 7 or more locations? What about in 2010:

``` r
brfss_2002 = 
  brfss_df %>% 
  filter(year == 2002) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)

brfss_2010 = 
  brfss_df %>% 
  filter(year == 2010) %>% 
  select(state, location) %>% 
  group_by(state) %>% 
  summarize(n_locations = n()) %>% 
  filter(n_locations >= 7)
```

-   In 2002, there were 36 states observed at 7 or more locations, they
    were AZ, CO, CT, DE, FL, GA, HI, ID, IL, IN, KS, LA, MA, MD, ME, MI,
    MN, MO, NC, NE, NH, NJ, NV, NY, OH, OK, OR, PA, RI, SC, SD, TN, TX,
    UT, VT, WA. In 2010, there were 45 states observed at 7 or more
    locations: AL, AR, AZ, CA, CO, CT, DE, FL, GA, HI, IA, ID, IL, IN,
    KS, LA, MA, MD, ME, MI, MN, MO, MS, MT, NC, ND, NE, NH, NJ, NM, NV,
    NY, OH, OK, OR, PA, RI, SC, SD, TN, TX, UT, VT, WA, WY.

#### Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data\_value across locations within a state:

``` r
excellent_df = 
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(data_value_mean = mean(data_value, na.rm = TRUE))
excellent_df
```

    ## # A tibble: 443 × 3
    ## # Groups:   year [9]
    ##     year state data_value_mean
    ##    <int> <chr>           <dbl>
    ##  1  2002 AK               27.9
    ##  2  2002 AL               18.5
    ##  3  2002 AR               24.1
    ##  4  2002 AZ               24.1
    ##  5  2002 CA               22.7
    ##  6  2002 CO               23.1
    ##  7  2002 CT               29.1
    ##  8  2002 DC               29.3
    ##  9  2002 DE               20.9
    ## 10  2002 FL               25.7
    ## # … with 433 more rows

-   The excellent response data set contains average data values of 51
    states (including DC) from 2002 to 2010.

#### Make a “spaghetti” plot of this average value over time within a state:

``` r
excellent_df %>% 
  ggplot(aes(x = year, y = data_value_mean, group = state, color = state)) +
  geom_line(alpha = 0.8) +
  labs(
    title = "Average Data Values Over Time",
    x = "Year",
    y = "Average Data Values",
    color = "State"
  ) +
  theme(legend.position = "right")
```

<img src="p8105_hw3_ly2565_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

-   The plot shows that average data values of all states fluctuate from
    2002 to 2010. Overall, there is a slight decreasing trend.

#### Make a two-panel plot showing, for the years 2006, and 2010, distribution of data\_value for responses (“Poor” to “Excellent”) among locations in NY State:

``` r
brfss_df %>% 
  filter(year %in% c(2006, 2010), state == "NY") %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Data Values for Responses in NY State",
    x = "Response",
    y = "Data Values"
  ) +
  facet_grid(. ~ year)
```

<img src="p8105_hw3_ly2565_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

-   From the plot, we can see that data values for positive
    responses(excellent, very good, good) were greater than those for
    fair and poor responses in both 2006 and 2010.

# Problem 3

## Load and tidy the accelerometer data set.

``` r
accelerometer_df = 
  read_csv(file = "data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekday_or_end = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday")) %>% 
    select(day_id, week, day, weekday_or_end, everything()) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(minute = as.numeric(minute))
```

    ## Rows: 35 Columns: 1443

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
accelerometer_df
```

    ## # A tibble: 50,400 × 6
    ##    day_id  week day    weekday_or_end minute activity_count
    ##     <dbl> <dbl> <chr>  <chr>           <dbl>          <dbl>
    ##  1      1     1 Friday Weekday             1           88.4
    ##  2      1     1 Friday Weekday             2           82.2
    ##  3      1     1 Friday Weekday             3           64.4
    ##  4      1     1 Friday Weekday             4           70.0
    ##  5      1     1 Friday Weekday             5           75.0
    ##  6      1     1 Friday Weekday             6           66.3
    ##  7      1     1 Friday Weekday             7           53.8
    ##  8      1     1 Friday Weekday             8           47.8
    ##  9      1     1 Friday Weekday             9           55.5
    ## 10      1     1 Friday Weekday            10           43.0
    ## # … with 50,390 more rows

-   The resulting data set has 50400 observations of 6 variables:
    day\_id, week, day, weekday\_or\_end, minute, activity\_count. In
    the `minute` column, `1` means the first minute starting at
    midnight, `2` means the second minute and so on. The values in the
    `activity_count` column are the activity counts for each minute.

## Aggregate activity counts for each day.

``` r
accelerometer_df %>% 
  group_by(week, day) %>% 
  summarize(activity_total = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = activity_total
  ) %>% 
  select(week, Monday, Tuesday, Wednesday, Thursday, Friday, everything()) %>% 
  knitr::kable()
```

| week |    Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday | Sunday |
|-----:|----------:|---------:|----------:|---------:|---------:|---------:|-------:|
|    1 |  78828.07 | 307094.2 |    340115 | 355923.6 | 480542.6 |   376254 | 631105 |
|    2 | 295431.00 | 423245.0 |    440962 | 474048.0 | 568839.0 |   607175 | 422018 |
|    3 | 685910.00 | 381507.0 |    468869 | 371230.0 | 467420.0 |   382928 | 467052 |
|    4 | 409450.00 | 319568.0 |    434460 | 340291.0 | 154049.0 |     1440 | 260617 |
|    5 | 389080.00 | 367824.0 |    445366 | 549658.0 | 620860.0 |     1440 | 138421 |

-   It seems total activities increased during weekdays, but the trend
    is not strictly increasing. In addition, the total activities on
    Saturday in week 4 and 5 dropped significantly, which may need us to
    pay attention to.

## Make a plot showing the 24-hour activity time courses for each day.

``` r
accelerometer_df %>% 
  group_by(day, minute) %>% 
  summarize(avg_activity = mean(activity_count)) %>% 
  ggplot(aes(x = minute, y = avg_activity, color = day)) +
  geom_line(alpha = 0.6) + 
  labs(
    title = "Average 24-hour Activity Time Courses",
    x = "Time",
    y = "Average Activity",
    color = "Day"
  ) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440), 
    labels = c("12am", "4am", "8am", "12pm", "4pm", "8pm", "11:59pm"),
    limits = c(0, 1440)
  )
```

<img src="p8105_hw3_ly2565_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

-   From the plot, we can see that of the 5 weeks, the average activity
    counts of each day tend to be lower from 10pm to 6am which may be
    the sleeping period; the average activity tend to be higher in 3
    periods of time on each day: 6am-12pm, 4pm-6pm, and 8pm-10pm.
    Especially on Fridays, this man was much more active from 8pm to
    10pm, and on Sundays, he was much more active from 10am-12pm.
